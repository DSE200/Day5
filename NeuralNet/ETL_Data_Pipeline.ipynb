{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL Data Pipeline: Extract, Transform, Load\n",
    "\n",
    "This notebook demonstrates a complete ETL workflow using a movies dataset from Kaggle:\n",
    "- **Extract**: Load data using kagglehub\n",
    "- **Transform**: Clean and process data through multiple steps\n",
    "- **Load**: Save processed data in different formats\n",
    "\n",
    "Each transformation step is visualized to show its impact on the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import kagglehub\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EXTRACT: Load Data from Kaggle\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Extract: Download movies dataset using kagglehub\n",
    "path = kagglehub.dataset_download(\"bharatnatrayn/movies-dataset-for-feature-extracion-prediction\")\n",
    "print(\"Path to dataset files:\", path)\n",
    "\n",
    "# Load the movies dataset\n",
    "data_path = Path(path) / \"movies.csv\"\n",
    "raw_df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"Dataset loaded: {raw_df.shape}\")\n",
    "print(f\"Columns: {list(raw_df.columns)}\")\n",
    "print(f\"Missing values: {raw_df.isnull().sum().sum()}\")\n",
    "raw_df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize raw data characteristics\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Missing data pattern\n",
    "sns.heatmap(raw_df.isnull(), ax=axes[0], cbar=True, yticklabels=False, cmap='viridis')\n",
    "axes[0].set_title('Raw Data: Missing Values')\n",
    "\n",
    "# Data types distribution\n",
    "dtype_counts = raw_df.dtypes.value_counts()\n",
    "axes[1].pie(dtype_counts.values, labels=dtype_counts.index, autopct='%1.0f%%')\n",
    "axes[1].set_title('Raw Data: Data Types')\n",
    "\n",
    "# Rating distribution\n",
    "if 'RATING' in raw_df.columns:\n",
    "    axes[2].hist(raw_df['RATING'].dropna(), bins=30, alpha=0.7, color='blue')\n",
    "    axes[2].set_title('Raw Data: Rating Distribution')\n",
    "    axes[2].set_xlabel('Rating')\n",
    "else:\n",
    "    axes[2].text(0.5, 0.5, 'No rating column found', ha='center', va='center')\n",
    "    axes[2].set_title('Raw Data: Sample Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFORM: Data Processing Pipeline\n",
    "\n",
    "Clean and process the raw data through multiple transformation steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Initialize transformation tracking\n",
    "df = raw_df.copy()\n",
    "transformations = []\n",
    "\n",
    "def track_transformation(step_name, before_df, after_df):\n",
    "    \"\"\"Track how each transformation affects the dataset\"\"\"\n",
    "    change = {\n",
    "        'Step': step_name,\n",
    "        'Rows': f\"{len(before_df)} → {len(after_df)}\",\n",
    "        'Columns': f\"{len(before_df.columns)} → {len(after_df.columns)}\",\n",
    "        'Missing': f\"{before_df.isnull().sum().sum()} → {after_df.isnull().sum().sum()}\",\n",
    "        'Rows_After': len(after_df),\n",
    "        'Cols_After': len(after_df.columns),\n",
    "        'Missing_After': after_df.isnull().sum().sum()\n",
    "    }\n",
    "    transformations.append(change)\n",
    "    print(f\"{step_name}: {change['Rows']}, {change['Missing']} missing\")\n",
    "\n",
    "print(\"Starting transformation pipeline...\")\n",
    "print(f\"Original data: {df.shape}, {df.isnull().sum().sum()} missing values\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 1: Clean column names and remove unnecessary columns\n",
    "before_df = df.copy()\n",
    "\n",
    "# Standardize column names\n",
    "df.columns = df.columns.str.upper().str.strip()\n",
    "\n",
    "# Remove rows with missing essential data (movies without names)\n",
    "df = df.dropna(subset=['MOVIES'])\n",
    "\n",
    "track_transformation('1. Clean names & remove missing movies', before_df, df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 2: Further clean up missing values\n",
    "before_df = df.copy()\n",
    "\n",
    "# we may want a better clean up strategy which we will discuss in the future weeks\n",
    "# for now drop gross column since it has too many missing values\n",
    "df = df.drop(columns=['GROSS'])\n",
    "\n",
    "# drop rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "track_transformation('2. Handle missing values', before_df, df)\n",
    "\n",
    "df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 3: Feature engineering\n",
    "before_df = df.copy()\n",
    "\n",
    "# Clean text fields - remove newlines and extra spaces\n",
    "text_columns = ['ONE-LINE', 'GENRE', 'STARS']\n",
    "for col in text_columns:\n",
    "    if col in df.columns:\n",
    "        df[col] = df[col].astype(str).str.replace(r'\\n', ' ', regex=True)\n",
    "        df[col] = df[col].str.replace(r'\\s+', ' ', regex=True)\n",
    "        df[col] = df[col].str.strip()\n",
    "\n",
    "# Parse YEAR field to handle ranges like (2021-) and (2010-2022)\n",
    "if 'YEAR' in df.columns:\n",
    "    # Extract start year from patterns like (2020-2022) or (2021-)\n",
    "    # Regex: \\((\\d{4})\n",
    "    # - \\( matches literal opening parenthesis\n",
    "    # - (\\d{4}) is a capture group that matches exactly 4 digits\n",
    "    # - The parentheses create a capture group that extract() returns\n",
    "    df['YEAR_START'] = df['YEAR'].astype(str).str.extract(r'\\((\\d{4})', expand=False).astype(float)\n",
    "    \n",
    "    # Extract end year if it exists\n",
    "    # Regex: (\\d{4})\\)\n",
    "    # - (\\d{4}) is a capture group that matches exactly 4 digits\n",
    "    # - \\) matches literal closing parenthesis\n",
    "    # - Only the captured group (the 4 digits) is returned, not the parenthesis\n",
    "    df['YEAR_END'] = df['YEAR'].astype(str).str.extract(r'(\\d{4})\\)', expand=False).astype(float)\n",
    "    \n",
    "    # For ongoing series (like 2021-), set end year to None\n",
    "    # Regex: \\d{4}–\\s*\\)\n",
    "    # - \\d{4} matches exactly 4 digits (not captured, just matched)\n",
    "    # - – matches the dash character\n",
    "    # - \\s* matches zero or more whitespace characters\n",
    "    # - \\) matches literal closing parenthesis\n",
    "    # - contains() returns True if pattern is found anywhere in string\n",
    "    ongoing_mask = df['YEAR'].str.contains(r'\\d{4}–\\s*\\)', na=False)\n",
    "    df.loc[ongoing_mask, 'YEAR_END'] = None\n",
    "    \n",
    "    # Create decade from start year\n",
    "    df['DECADE'] = (df['YEAR_START'] // 10 * 10).astype('Int64')\n",
    "\n",
    "    # drop year column now that we have the start and end years\n",
    "    df = df.drop(columns=['YEAR'])\n",
    "\n",
    "# Parse GENRE field to extract all genres\n",
    "if 'GENRE' in df.columns:\n",
    "    # Split genres and clean them\n",
    "    df['GENRES_LIST'] = df['GENRE'].str.split(',')\n",
    "    df['GENRES_LIST'] = df['GENRES_LIST'].apply(\n",
    "        # lambda means execute this function for each element in the list\n",
    "        # x is the element in the list\n",
    "        # if x is a list, execute the following code\n",
    "        #   for each element in the list, execute the following code\n",
    "        #       strip the element of whitespace\n",
    "        # if x is not a list, return an empty list\n",
    "        lambda x: [genre.strip() for genre in x] if isinstance(x, list) else []\n",
    "    )\n",
    "    \n",
    "    # Extract primary genre (first one)\n",
    "    df['PRIMARY_GENRE'] = df['GENRES_LIST'].apply(\n",
    "        # lambda means execute this function for each element in the list\n",
    "        # x is the element in the list\n",
    "        # if x is a list, execute the following code\n",
    "        #   if x is not empty and has more than 0 elements, return the first element\n",
    "        #   otherwise return 'Unknown'\n",
    "        lambda x: x[0] if x and len(x) > 0 else 'Unknown'\n",
    "    )\n",
    "    \n",
    "    # Count number of genres\n",
    "    df['GENRE_COUNT'] = df['GENRES_LIST'].apply(len)\n",
    "\n",
    "    # drop genre column now that we have the primary genre and genre count\n",
    "    df = df.drop(columns=['GENRE'])\n",
    "\n",
    "# Parse STARS field to extract director and main stars\n",
    "if 'STARS' in df.columns:\n",
    "    # Extract director\n",
    "    # Regex: Director:\\s*([^|]+)\n",
    "    # - Director: matches the literal text \"Director:\"\n",
    "    # - \\s* matches zero or more whitespace characters after \"Director:\"\n",
    "    # - ([^|]+) is a capture group that matches one or more characters that are NOT pipe (|)\n",
    "    #   The pipe character is used as a delimiter in the STARS field to separate Director from Stars sections\n",
    "    # - The parentheses create a capture group, and extract() returns only what's inside the parentheses\n",
    "    # - This captures everything after \"Director:\" until it hits a pipe character or end of string\n",
    "    df['DIRECTOR'] = df['STARS'].str.extract(r'Director:\\s*([^|]+)', expand=False)\n",
    "    df['DIRECTOR'] = df['DIRECTOR'].str.strip()\n",
    "    \n",
    "    # Extract main stars (after \"Stars:\" until next delimiter or end)\n",
    "    # Regex: Stars:\\s*([^|]+)\n",
    "    # - Stars: matches the literal text \"Stars:\"\n",
    "    # - \\s* matches zero or more whitespace characters after \"Stars:\"\n",
    "    # - ([^|]+) is a capture group that matches one or more characters that are NOT pipe (|)\n",
    "    # - The capture group extracts all star names after \"Stars:\" until a pipe or end of string\n",
    "    df['MAIN_STARS'] = df['STARS'].str.extract(r'Stars:\\s*([^|]+)', expand=False)\n",
    "    df['MAIN_STARS'] = df['MAIN_STARS'].str.strip()\n",
    "    \n",
    "    # Count number of stars mentioned\n",
    "    df['STAR_COUNT'] = df['MAIN_STARS'].str.count(',') + 1\n",
    "    df.loc[df['MAIN_STARS'].isna(), 'STAR_COUNT'] = 0\n",
    "\n",
    "    # drop stars column now that we have the main stars and star count\n",
    "    df = df.drop(columns=['STARS'])\n",
    "\n",
    "track_transformation('3. Advanced cleaning & parsing', before_df, df)\n",
    "\n",
    "df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 3.5: Categorical Encoding - Preserve All Data\n",
    "before_df = df.copy()\n",
    "\n",
    "# ONE-HOT ENCODING FOR GENRES (preserve all genre information)\n",
    "if 'GENRES_LIST' in df.columns:\n",
    "    # Get all unique genres and their frequency\n",
    "    all_genres = {}\n",
    "    for genre_list in df['GENRES_LIST'].dropna():\n",
    "        for genre in genre_list:\n",
    "            if genre and genre != 'nan':\n",
    "                # .get(genre, 0) returns the current count for genre, or 0 if genre doesn't exist yet\n",
    "                # This counts how many times each genre appears across all movies\n",
    "                all_genres[genre] = all_genres.get(genre, 0) + 1\n",
    "    \n",
    "    # Keep top 20 most common genres for one-hot encoding (balance between completeness and dimensionality)\n",
    "    # sorted() sorts the dictionary items\n",
    "    # key=lambda x: x[1] means sort by the second element of each tuple (the count)\n",
    "    # reverse=True means sort in descending order (highest count first)\n",
    "    # [:20] takes only the first 20 items\n",
    "    top_genres = sorted(all_genres.items(), key=lambda x: x[1], reverse=True)[:20]\n",
    "    top_genre_names = [genre for genre, count in top_genres]\n",
    "    \n",
    "    print(f\"Creating one-hot encoding for top {len(top_genre_names)} genres\")\n",
    "    print(f\"Top 10 genres: {', '.join(top_genre_names[:10])}\")\n",
    "    \n",
    "    # Create binary columns for each top genre\n",
    "    for genre in top_genre_names:\n",
    "        # Replace special characters to create valid column names\n",
    "        # .replace() substitutes characters that might cause issues in column names\n",
    "        safe_genre_name = genre.upper().replace(' ', '_').replace('-', '_').replace('(', '').replace(')', '')\n",
    "        # lambda x: checks if x is a list and if genre is in that list\n",
    "        # Returns 1 if genre is present, 0 otherwise (binary encoding)\n",
    "        df[f'GENRE_{safe_genre_name}'] = df['GENRES_LIST'].apply(\n",
    "            lambda x: 1 if isinstance(x, list) and genre in x else 0\n",
    "        )\n",
    "\n",
    "# EXTRACT AND ENCODE STAR INFORMATION (preserve individual star data)\n",
    "if 'MAIN_STARS' in df.columns:\n",
    "    # Extract individual star names\n",
    "    # .str.split(',') splits the string by comma into a list\n",
    "    df['STARS_LIST'] = df['MAIN_STARS'].str.split(',')\n",
    "    # Clean up the star names\n",
    "    # lambda x: processes each list x\n",
    "    # [star.strip() for star in x if star.strip()] removes whitespace and filters out empty strings\n",
    "    # isinstance(x, list) checks if x is actually a list before processing\n",
    "    df['STARS_LIST'] = df['STARS_LIST'].apply(\n",
    "        lambda x: [star.strip() for star in x if star.strip()] if isinstance(x, list) else []\n",
    "    )\n",
    "    \n",
    "    # Get top actors/actresses\n",
    "    all_stars = {}\n",
    "    for star_list in df['STARS_LIST'].dropna():\n",
    "        for star in star_list:\n",
    "            # len(star) > 2 filters out initials or very short names that might be errors\n",
    "            if star and star != 'nan' and len(star) > 2:\n",
    "                all_stars[star] = all_stars.get(star, 0) + 1\n",
    "    \n",
    "    # Keep top 15 most frequent stars for encoding\n",
    "    top_stars = sorted(all_stars.items(), key=lambda x: x[1], reverse=True)[:15]\n",
    "    # Filter to only include stars that appear in 3 or more movies\n",
    "    # This ensures we're encoding meaningful patterns, not one-off appearances\n",
    "    top_star_names = [star for star, count in top_stars if count >= 3]\n",
    "    \n",
    "    print(f\"Creating encoding for top {len(top_star_names)} stars\")\n",
    "    print(f\"Top 5 stars: {', '.join(top_star_names[:5])}\")\n",
    "    \n",
    "    # Create binary columns for each top star\n",
    "    for star in top_star_names:\n",
    "        safe_star_name = star.upper().replace(' ', '_').replace('-', '_').replace('.', '')\n",
    "        # lambda x: checks if star appears in the movie's star list\n",
    "        # Returns 1 if star is in the movie, 0 otherwise\n",
    "        df[f'STAR_{safe_star_name}'] = df['STARS_LIST'].apply(\n",
    "            lambda x: 1 if isinstance(x, list) and star in x else 0\n",
    "        )\n",
    "\n",
    "# DIRECTOR ENCODING (preserve director information)\n",
    "if 'DIRECTOR' in df.columns:\n",
    "    # Get director frequency\n",
    "    # .value_counts() counts how many times each unique director appears\n",
    "    director_counts = df['DIRECTOR'].value_counts()\n",
    "    # Keep directors with 2+ movies\n",
    "    # director_counts >= 2 creates a boolean mask\n",
    "    # .index.tolist() gets the director names (index) as a list\n",
    "    frequent_directors = director_counts[director_counts >= 2].index.tolist()\n",
    "    \n",
    "    print(f\"Creating encoding for {len(frequent_directors)} frequent directors\")\n",
    "    \n",
    "    # Create binary columns for frequent directors\n",
    "    for director in frequent_directors[:10]:  # Top 10 most prolific directors\n",
    "        # pd.notna() checks if director is not NaN (missing value)\n",
    "        if director and pd.notna(director):\n",
    "            safe_director_name = director.upper().replace(' ', '_').replace('-', '_').replace('.', '')\n",
    "            # (df['DIRECTOR'] == director) creates a boolean series\n",
    "            # .astype(int) converts True to 1 and False to 0\n",
    "            df[f'DIRECTOR_{safe_director_name}'] = (df['DIRECTOR'] == director).astype(int)\n",
    "\n",
    "track_transformation('3.5. Categorical encoding (preserve all data)', before_df, df)\n",
    "\n",
    "df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Step 4: Clean and standardize text data\n",
    "before_df = df.copy()\n",
    "\n",
    "# Create rating categories\n",
    "if 'RATING' in df.columns:\n",
    "    df['RATING_CATEGORY'] = pd.cut(df['RATING'], \n",
    "                                   bins=[0, 4, 6, 8, 10], \n",
    "                                   labels=['Poor', 'Average', 'Good', 'Excellent'])\n",
    "\n",
    "# Create runtime categories\n",
    "if 'RUNTIME' in df.columns:\n",
    "    df['RUNTIME_CATEGORY'] = pd.cut(df['RUNTIME'], \n",
    "                                    bins=[0, 90, 120, 180, 1000], \n",
    "                                    labels=['Short', 'Standard', 'Long', 'Epic'])\n",
    "\n",
    "# Standardize text formatting\n",
    "if 'MOVIES' in df.columns:\n",
    "    df['MOVIES'] = df['MOVIES'].str.strip()\n",
    "\n",
    "if 'PRIMARY_GENRE' in df.columns:\n",
    "    df['PRIMARY_GENRE'] = df['PRIMARY_GENRE'].str.title()\n",
    "\n",
    "if 'DIRECTOR' in df.columns:\n",
    "    df['DIRECTOR'] = df['DIRECTOR'].str.title()\n",
    "\n",
    "# Create movie age based on start year\n",
    "if 'YEAR_START' in df.columns:\n",
    "    current_year = 2025\n",
    "    df['MOVIE_AGE'] = current_year - df['YEAR_START']\n",
    "\n",
    "# Categorize by series vs movie (based on year range)\n",
    "if 'YEAR_END' in df.columns:\n",
    "    df['IS_SERIES'] = df['YEAR_END'].notna()\n",
    "    \n",
    "# Remove obvious outliers\n",
    "if 'RATING' in df.columns:\n",
    "    df = df[(df['RATING'] >= 1) & (df['RATING'] <= 10)]\n",
    "\n",
    "if 'RUNTIME' in df.columns:\n",
    "    # Remove movies with unrealistic runtime (less than 1 min or more than 10 hours)\n",
    "    df = df[(df['RUNTIME'] >= 1) & (df['RUNTIME'] <= 600)]\n",
    "\n",
    "track_transformation('4. Feature engineering & standardization', before_df, df)\\\n",
    "\n",
    "df.head()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visualize transformation pipeline effects\n",
    "transformations_df = pd.DataFrame(transformations)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Dataset shape changes\n",
    "x = range(len(transformations_df))\n",
    "axes[0,0].plot(x, transformations_df['Rows_After'], 'o-', label='Rows', linewidth=2, markersize=8)\n",
    "axes[0,0].plot(x, transformations_df['Cols_After'], 's-', label='Columns', linewidth=2, markersize=8)\n",
    "axes[0,0].set_title('Dataset Shape Through Pipeline')\n",
    "axes[0,0].set_xlabel('Transformation Step')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "step_labels = [f\"Step {i+1}\" for i in x]\n",
    "axes[0,0].set_xticks(x)\n",
    "axes[0,0].set_xticklabels(step_labels)\n",
    "\n",
    "# Missing values reduction\n",
    "axes[0,1].bar(x, transformations_df['Missing_After'], color='red', alpha=0.7)\n",
    "axes[0,1].set_title('Missing Values After Each Step')\n",
    "axes[0,1].set_xlabel('Transformation Step')\n",
    "axes[0,1].set_ylabel('Missing Values')\n",
    "axes[0,1].set_xticks(x)\n",
    "axes[0,1].set_xticklabels(step_labels)\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "\n",
    "# Before/After comparison\n",
    "comparison_data = {\n",
    "    'Metric': ['Rows', 'Columns', 'Missing Values'],\n",
    "    'Original': [len(raw_df), len(raw_df.columns), raw_df.isnull().sum().sum()],\n",
    "    'Final': [len(df), len(df.columns), df.isnull().sum().sum()]\n",
    "}\n",
    "comp_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "x_pos = range(len(comp_df))\n",
    "width = 0.35\n",
    "axes[1,0].bar([x - width/2 for x in x_pos], comp_df['Original'], width, label='Original', alpha=0.8)\n",
    "axes[1,0].bar([x + width/2 for x in x_pos], comp_df['Final'], width, label='Final', alpha=0.8)\n",
    "axes[1,0].set_title('Original vs Final Dataset')\n",
    "axes[1,0].set_ylabel('Count')\n",
    "axes[1,0].set_xticks(x_pos)\n",
    "axes[1,0].set_xticklabels(comp_df['Metric'])\n",
    "axes[1,0].legend()\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# Transformation summary table\n",
    "axes[1,1].axis('tight')\n",
    "axes[1,1].axis('off')\n",
    "table_data = transformations_df[['Step', 'Rows', 'Columns', 'Missing']]\n",
    "table = axes[1,1].table(cellText=table_data.values, colLabels=table_data.columns, \n",
    "                        cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False)\n",
    "table.set_fontsize(9)\n",
    "table.scale(1.2, 1.5)\n",
    "axes[1,1].set_title('Transformation Summary')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nPipeline completed:\")\n",
    "print(f\"Original: {raw_df.shape} → Final: {df.shape}\")\n",
    "print(f\"Missing values: {raw_df.isnull().sum().sum()} → {df.isnull().sum().sum()}\")\n",
    "print(f\"New features created: {len(df.columns) - len(raw_df.columns)}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD: Save Processed Data\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Save processed data in multiple formats\n",
    "output_dir = Path('processed_data')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"SAVING PROCESSED DATA\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Save as CSV\n",
    "csv_path = output_dir / 'movies_processed.csv'\n",
    "df.to_csv(csv_path, index=False)\n",
    "csv_size = csv_path.stat().st_size / 1024\n",
    "print(f\"CSV saved: {csv_size:.1f} KB\")\n",
    "\n",
    "# Save as Parquet (more efficient)\n",
    "parquet_path = output_dir / 'movies_processed.parquet'\n",
    "df.to_parquet(parquet_path, index=False)\n",
    "parquet_size = parquet_path.stat().st_size / 1024\n",
    "print(f\"Parquet saved: {parquet_size:.1f} KB\")\n",
    "\n",
    "# Save transformation metadata\n",
    "metadata = {\n",
    "    'original_shape': raw_df.shape,\n",
    "    'final_shape': df.shape,\n",
    "    'transformations_applied': len(transformations),\n",
    "    'missing_values_original': raw_df.isnull().sum().sum(),\n",
    "    'missing_values_final': df.isnull().sum().sum(),\n",
    "    'compression_ratio': f\"{parquet_size/csv_size:.2f}\"\n",
    "}\n",
    "\n",
    "metadata_df = pd.DataFrame(list(metadata.items()), columns=['Metric', 'Value'])\n",
    "metadata_path = output_dir / 'transformation_metadata.csv'\n",
    "metadata_df.to_csv(metadata_path, index=False)\n",
    "\n",
    "print(f\"Metadata saved\")\n",
    "print(f\"Compression ratio (Parquet vs CSV): {parquet_size/csv_size:.2f}\")\n",
    "\n",
    "display(metadata_df)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Show final dataset characteristics and new features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Rating distribution (before vs after)\n",
    "if 'RATING' in raw_df.columns and 'RATING' in df.columns:\n",
    "    axes[0,0].hist(raw_df['RATING'].dropna(), bins=20, alpha=0.7, label='Original', color='red')\n",
    "    axes[0,0].hist(df['RATING'].dropna(), bins=20, alpha=0.7, label='Processed', color='blue')\n",
    "    axes[0,0].set_title('Rating Distribution: Before vs After')\n",
    "    axes[0,0].set_xlabel('Rating')\n",
    "    axes[0,0].legend()\n",
    "\n",
    "# New feature: Genre count distribution\n",
    "if 'GENRE_COUNT' in df.columns:\n",
    "    genre_counts = df['GENRE_COUNT'].value_counts().sort_index()\n",
    "    axes[0,1].bar(genre_counts.index, genre_counts.values, color='purple', alpha=0.7)\n",
    "    axes[0,1].set_title('Number of Genres per Movie')\n",
    "    axes[0,1].set_xlabel('Genre Count')\n",
    "    axes[0,1].set_ylabel('Number of Movies')\n",
    "\n",
    "# Series vs Movies\n",
    "if 'IS_SERIES' in df.columns:\n",
    "    series_counts = df['IS_SERIES'].value_counts()\n",
    "    axes[0,2].pie(series_counts.values, labels=['Movies', 'Series'], autopct='%1.1f%%', \n",
    "                 colors=['lightblue', 'lightcoral'])\n",
    "    axes[0,2].set_title('Movies vs Series Distribution')\n",
    "\n",
    "# Primary genres distribution\n",
    "if 'PRIMARY_GENRE' in df.columns:\n",
    "    top_genres = df['PRIMARY_GENRE'].value_counts().head(10)\n",
    "    axes[1,0].barh(top_genres.index, top_genres.values, color='orange', alpha=0.7)\n",
    "    axes[1,0].set_title('Top 10 Primary Genres')\n",
    "    axes[1,0].set_xlabel('Count')\n",
    "\n",
    "# Movie age distribution\n",
    "if 'MOVIE_AGE' in df.columns:\n",
    "    axes[1,1].hist(df['MOVIE_AGE'].dropna(), bins=20, color='green', alpha=0.7)\n",
    "    axes[1,1].set_title('Movie Age Distribution')\n",
    "    axes[1,1].set_xlabel('Years Since Release')\n",
    "    axes[1,1].set_ylabel('Count')\n",
    "\n",
    "# Data completeness for key parsed fields\n",
    "new_fields = ['DIRECTOR', 'MAIN_STARS', 'YEAR_START', 'PRIMARY_GENRE']\n",
    "completeness_new = {}\n",
    "for field in new_fields:\n",
    "    if field in df.columns:\n",
    "        completeness_new[field] = (1 - df[field].isnull().sum() / len(df)) * 100\n",
    "\n",
    "if completeness_new:\n",
    "    axes[1,2].bar(completeness_new.keys(), completeness_new.values(), color='teal', alpha=0.7)\n",
    "    axes[1,2].set_title('Completeness of Parsed Fields')\n",
    "    axes[1,2].set_ylabel('Completeness %')\n",
    "    axes[1,2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display sample of processed data showing new fields\n",
    "print(\"SAMPLE OF PROCESSED DATA WITH NEW FIELDS:\")\n",
    "print(\"=\" * 50)\n",
    "sample_cols = ['MOVIES', 'YEAR_START', 'YEAR_END', 'PRIMARY_GENRE', 'GENRE_COUNT', \n",
    "               'DIRECTOR', 'RATING', 'IS_SERIES', 'MOVIE_AGE']\n",
    "available_cols = [col for col in sample_cols if col in df.columns]\n",
    "print(df[available_cols].head())\n",
    "\n",
    "# Show categorical encoding results\n",
    "print(f\"\\nCATEGORICAL ENCODING RESULTS:\")\n",
    "print(\"=\" * 40)\n",
    "genre_cols = [col for col in df.columns if col.startswith('GENRE_')]\n",
    "star_cols = [col for col in df.columns if col.startswith('STAR_')]\n",
    "director_cols = [col for col in df.columns if col.startswith('DIRECTOR_')]\n",
    "\n",
    "print(f\"Genre one-hot columns created: {len(genre_cols)}\")\n",
    "print(f\"Star encoding columns created: {len(star_cols)}\")\n",
    "print(f\"Director encoding columns created: {len(director_cols)}\")\n",
    "\n",
    "# Show example of multi-genre movie\n",
    "if genre_cols:\n",
    "    print(f\"\\nExample multi-genre movie:\")\n",
    "    for idx, row in df.head().iterrows():\n",
    "        genre_values = [(col.replace('GENRE_', ''), row[col]) for col in genre_cols if row[col] == 1]\n",
    "        if len(genre_values) > 1:\n",
    "            print(f\"'{row['MOVIES']}': {[g[0] for g in genre_values]}\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nETL Pipeline completed successfully!\")\n",
    "print(f\"Original dataset: {raw_df.shape}\")\n",
    "print(f\"Final dataset: {df.shape}\")\n",
    "print(f\"New features created: {len(df.columns) - len(raw_df.columns)}\")\n",
    "print(f\"Files saved in: {output_dir.absolute()}\")\n",
    "\n",
    "# Show parsing success rates\n",
    "print(f\"\\nPARSING SUCCESS RATES:\")\n",
    "if 'DIRECTOR' in df.columns:\n",
    "    director_success = (1 - df['DIRECTOR'].isnull().sum() / len(df)) * 100\n",
    "    print(f\"Director extraction: {director_success:.1f}%\")\n",
    "if 'YEAR_START' in df.columns:\n",
    "    year_success = (1 - df['YEAR_START'].isnull().sum() / len(df)) * 100\n",
    "    print(f\"Year parsing: {year_success:.1f}%\")\n",
    "if 'PRIMARY_GENRE' in df.columns:\n",
    "    genre_success = (df['PRIMARY_GENRE'] != 'Unknown').sum() / len(df) * 100\n",
    "    print(f\"Genre parsing: {genre_success:.1f}%\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install scikit-learn"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# BONUS: Quick Feature Importance Analysis\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Prepare features for analysis\n",
    "feature_cols = []\n",
    "\n",
    "# Add numerical features\n",
    "numerical_features = ['RUNTIME', 'VOTES', 'YEAR_START', 'MOVIE_AGE', 'GENRE_COUNT', 'STAR_COUNT']\n",
    "for col in numerical_features:\n",
    "    if col in df.columns:\n",
    "        feature_cols.append(col)\n",
    "\n",
    "# Add categorical features (limit to top 15 to avoid overfitting)\n",
    "categorical_features = [col for col in df.columns if col.startswith(('GENRE_', 'STAR_', 'DIRECTOR_'))]\n",
    "feature_cols.extend(categorical_features[:15])\n",
    "\n",
    "# Remove any duplicate columns\n",
    "feature_cols = list(dict.fromkeys(feature_cols))\n",
    "\n",
    "# Prepare data for analysis\n",
    "analysis_df = df[feature_cols + ['RATING', 'MOVIES']].dropna()\n",
    "\n",
    "# Convert VOTES column to numeric if it's still string\n",
    "if 'VOTES' in analysis_df.columns:\n",
    "    analysis_df['VOTES'] = pd.to_numeric(analysis_df['VOTES'].astype(str).str.replace(',', ''), errors='coerce')\n",
    "\n",
    "# Ensure we have valid feature columns that exist in the dataframe\n",
    "valid_feature_cols = [col for col in feature_cols if col in analysis_df.columns]\n",
    "\n",
    "X = analysis_df[valid_feature_cols]\n",
    "y = analysis_df['RATING']\n",
    "\n",
    "# Train Random Forest model\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create feature importance dataframe\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': valid_feature_cols,\n",
    "    'Importance': importances\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"Analysis based on {len(analysis_df)} movies with complete data\")\n",
    "print(f\"Model R² Score: {rf.score(X, y):.3f}\")\n",
    "print(\"\\nTOP 10 MOST IMPORTANT FEATURES FOR PREDICTING RATING:\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows(), 1):\n",
    "    feature_name = row['Feature']\n",
    "    importance = row['Importance']\n",
    "    \n",
    "    if feature_name.startswith('GENRE_'):\n",
    "        display_name = f\"Genre: {feature_name.replace('GENRE_', '').replace('_', ' ').title()}\"\n",
    "    elif feature_name.startswith('STAR_'):\n",
    "        display_name = f\"Star: {feature_name.replace('STAR_', '').replace('_', ' ').title()}\"\n",
    "    elif feature_name.startswith('DIRECTOR_'):\n",
    "        display_name = f\"Director: {feature_name.replace('DIRECTOR_', '').replace('_', ' ').title()}\"\n",
    "    else:\n",
    "        display_name = feature_name.replace('_', ' ').title()\n",
    "    \n",
    "    print(f\"{i:2d}. {display_name:<35} {importance:.3f}\")\n",
    "\n",
    "top_feature = feature_importance_df.iloc[0]\n",
    "print(f\"\\nINSIGHT: '{top_feature['Feature']}' is the strongest predictor of movie ratings!\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install torch\n",
    "%pip install plotly"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# bonus number 2 neural nets\n",
    "# Simple Neural Network for Rating Prediction\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values.reshape(-1, 1))\n",
    "X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "y_test_tensor = torch.FloatTensor(y_test.values.reshape(-1, 1))\n",
    "\n",
    "# Define neural network\n",
    "class RatingPredictor(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "# Function to get model parameters as vector\n",
    "def get_params_vector(model):\n",
    "    params = []\n",
    "    for p in model.parameters():\n",
    "        params.extend(p.data.cpu().numpy().flatten())\n",
    "    return np.array(params)\n",
    "\n",
    "# Function to set model parameters from vector\n",
    "def set_params_from_vector(model, params_vector):\n",
    "    idx = 0\n",
    "    for p in model.parameters():\n",
    "        param_length = p.numel()\n",
    "        p.data = torch.from_numpy(params_vector[idx:idx+param_length]).float().reshape(p.shape)\n",
    "        idx += param_length\n",
    "\n",
    "# Function to evaluate loss at given parameters\n",
    "def evaluate_loss(params_vector, model_template, X, y):\n",
    "    model = RatingPredictor(X.shape[1])\n",
    "    set_params_from_vector(model, params_vector)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X)\n",
    "        loss = nn.MSELoss()(predictions, y)\n",
    "    return loss.item()\n",
    "\n",
    "# Train model and collect trajectory\n",
    "print(\"Training neural network and collecting trajectory...\")\n",
    "model = RatingPredictor(X_train_scaled.shape[1])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Store trajectory\n",
    "trajectory_params = []\n",
    "trajectory_losses = []\n",
    "\n",
    "# Initial parameters\n",
    "initial_params = get_params_vector(model)\n",
    "trajectory_params.append(initial_params.copy())\n",
    "trajectory_losses.append(evaluate_loss(initial_params, model, X_train_tensor, y_train_tensor))\n",
    "\n",
    "# Train for 100 epochs\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    predictions = model(X_train_tensor)\n",
    "    loss = loss_fn(predictions, y_train_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Save trajectory every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        current_params = get_params_vector(model)\n",
    "        trajectory_params.append(current_params.copy())\n",
    "        trajectory_losses.append(loss.item())\n",
    "        \n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "\n",
    "# Final parameters\n",
    "final_params = get_params_vector(model)\n",
    "trajectory_params.append(final_params.copy())\n",
    "trajectory_losses.append(evaluate_loss(final_params, model, X_train_tensor, y_train_tensor))\n",
    "\n",
    "print(f\"\\nTraining complete! Final loss: {trajectory_losses[-1]:.4f}\")\n",
    "\n",
    "# Plot loss per epoch (simple 2D plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(0, len(trajectory_losses)*5, 5), trajectory_losses, 'b-', linewidth=2.5)\n",
    "plt.title('Training Loss per Epoch', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Epoch', fontsize=12)\n",
    "plt.ylabel('MSE Loss', fontsize=12)\n",
    "plt.yscale('log')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create 3D loss landscape visualization\n",
    "print(\"\\nGenerating interactive 3D loss landscape...\")\n",
    "\n",
    "\n",
    "trajectory_params_array = np.array(trajectory_params)\n",
    "pca = PCA(n_components=2)\n",
    "trajectory_2d = pca.fit_transform(trajectory_params_array)\n",
    "\n",
    "# Create grid around trajectory\n",
    "margin = 2.0\n",
    "x_min, x_max = trajectory_2d[:, 0].min() - margin, trajectory_2d[:, 0].max() + margin\n",
    "y_min, y_max = trajectory_2d[:, 1].min() - margin, trajectory_2d[:, 1].max() + margin\n",
    "\n",
    "# Create dense grid\n",
    "grid_size = 50\n",
    "x_range = np.linspace(x_min, x_max, grid_size)\n",
    "y_range = np.linspace(y_min, y_max, grid_size)\n",
    "X_grid, Y_grid = np.meshgrid(x_range, y_range)\n",
    "\n",
    "# Compute loss surface\n",
    "print(\"Computing loss surface...\")\n",
    "Z_grid = np.zeros((grid_size, grid_size))\n",
    "\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        # Get 2D point\n",
    "        point_2d = np.array([X_grid[i, j], Y_grid[i, j]])\n",
    "        \n",
    "        # Find closest trajectory point for interpolation\n",
    "        distances = np.sum((trajectory_2d - point_2d)**2, axis=1)\n",
    "        closest_idx = np.argmin(distances)\n",
    "        \n",
    "        # Interpolate in parameter space\n",
    "        if closest_idx == 0:\n",
    "            direction = trajectory_params[1] - trajectory_params[0]\n",
    "        elif closest_idx == len(trajectory_params) - 1:\n",
    "            direction = trajectory_params[-1] - trajectory_params[-2]\n",
    "        else:\n",
    "            direction = trajectory_params[closest_idx + 1] - trajectory_params[closest_idx - 1]\n",
    "        \n",
    "        # Project displacement onto parameter space\n",
    "        displacement_2d = point_2d - trajectory_2d[closest_idx]\n",
    "        \n",
    "        # Create perturbation in parameter space\n",
    "        perturbation = np.zeros_like(trajectory_params[0])\n",
    "        if len(pca.components_) >= 2:\n",
    "            perturbation += displacement_2d[0] * pca.components_[0] * np.std(trajectory_params_array[:, 0])\n",
    "            perturbation += displacement_2d[1] * pca.components_[1] * np.std(trajectory_params_array[:, 1])\n",
    "        \n",
    "        # Evaluate loss at perturbed point\n",
    "        perturbed_params = trajectory_params[closest_idx] + perturbation * 0.5\n",
    "        Z_grid[i, j] = evaluate_loss(perturbed_params, model, X_train_tensor, y_train_tensor)\n",
    "\n",
    "# Create interactive 3D plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add loss surface\n",
    "fig.add_trace(go.Surface(\n",
    "    x=x_range,\n",
    "    y=y_range,\n",
    "    z=Z_grid,\n",
    "    colorscale='Viridis',\n",
    "    opacity=0.8,\n",
    "    showscale=True,\n",
    "    colorbar=dict(\n",
    "        title=\"Loss\",\n",
    "        tickmode=\"linear\",\n",
    "        tick0=0,\n",
    "        dtick=0.5\n",
    "    ),\n",
    "    contours=dict(\n",
    "        z=dict(\n",
    "            show=True,\n",
    "            usecolormap=True,\n",
    "            highlightcolor=\"limegreen\",\n",
    "            project=dict(z=True)\n",
    "        )\n",
    "    ),\n",
    "    name='Loss Surface'\n",
    "))\n",
    "\n",
    "# Add optimization trajectory\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=trajectory_2d[:, 0],\n",
    "    y=trajectory_2d[:, 1],\n",
    "    z=trajectory_losses,\n",
    "    mode='lines+markers',\n",
    "    name='Optimization Path',\n",
    "    line=dict(\n",
    "        color='red',\n",
    "        width=10\n",
    "    ),\n",
    "    marker=dict(\n",
    "        size=8,\n",
    "        color='red',\n",
    "        symbol='circle'\n",
    "    )\n",
    "))\n",
    "\n",
    "# Add start point\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=[trajectory_2d[0, 0]],\n",
    "    y=[trajectory_2d[0, 1]],\n",
    "    z=[trajectory_losses[0]],\n",
    "    mode='markers',\n",
    "    name='Start',\n",
    "    marker=dict(\n",
    "        size=20,\n",
    "        color='green',\n",
    "        symbol='diamond'\n",
    "    )\n",
    "))\n",
    "\n",
    "# Add end point\n",
    "fig.add_trace(go.Scatter3d(\n",
    "    x=[trajectory_2d[-1, 0]],\n",
    "    y=[trajectory_2d[-1, 1]],\n",
    "    z=[trajectory_losses[-1]],\n",
    "    mode='markers',\n",
    "    name='End',\n",
    "    marker=dict(\n",
    "        size=20,\n",
    "        color='blue',\n",
    "        symbol='square'\n",
    "    )\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title=dict(\n",
    "        text='Interactive 3D Loss Landscape with Optimization Trajectory',\n",
    "        font=dict(size=24),\n",
    "        x=0.5,\n",
    "        xanchor='center'\n",
    "    ),\n",
    "    scene=dict(\n",
    "        xaxis=dict(\n",
    "            title='First Principal Component',\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            gridwidth=2\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title='Second Principal Component',\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            gridwidth=2\n",
    "        ),\n",
    "        zaxis=dict(\n",
    "            title='Loss',\n",
    "            gridcolor='rgb(255, 255, 255)',\n",
    "            gridwidth=2\n",
    "        ),\n",
    "        camera=dict(\n",
    "            eye=dict(x=1.5, y=1.5, z=1.2),\n",
    "            center=dict(x=0, y=0, z=0)\n",
    "        ),\n",
    "        aspectmode='manual',\n",
    "        aspectratio=dict(x=1, y=1, z=0.7)\n",
    "    ),\n",
    "    width=1400,\n",
    "    height=1000,\n",
    "    legend=dict(\n",
    "        x=0.02,\n",
    "        y=0.98,\n",
    "        bgcolor='rgba(255, 255, 255, 0.9)',\n",
    "        bordercolor='black',\n",
    "        borderwidth=2,\n",
    "        font=dict(size=14)\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, t=50, b=0)\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\nOptimization Summary:\")\n",
    "print(f\"Initial Loss: {trajectory_losses[0]:.4f}\")\n",
    "print(f\"Final Loss: {trajectory_losses[-1]:.4f}\")\n",
    "print(f\"Improvement: {(1 - trajectory_losses[-1]/trajectory_losses[0])*100:.1f}%\")\n",
    "print(f\"Number of trajectory points: {len(trajectory_losses)}\")\n",
    "\n",
    "print(\"\\nInteractive controls:\")\n",
    "print(\"- Rotate: Click and drag\")\n",
    "print(\"- Zoom: Scroll or pinch\")\n",
    "print(\"- Pan: Shift + click and drag\")\n",
    "print(\"- Reset: Double click\")\n",
    "print(\"- Hover over trajectory to see loss values\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

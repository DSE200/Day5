{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "%pip install kagglehub matplotlib nltk numpy pandas seaborn torch scikit-learn ipywidgets sentence-transformers streamlit nbformat spacy textblob\n",
    "\n",
    "try:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "except:\n",
    "    pass # Not running in Colab\n",
    "\n",
    "try:\n",
    "    !python3 -m spacy download en_core_web_sm\n",
    "except:\n",
    "    pass\n",
    "# top should work, adding others below for compatibility with differnet environments\n",
    "try:\n",
    "    import sys\n",
    "    !{sys.executable} -m spacy download en_core_web_sm\n",
    "except:\n",
    "    pass\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import seaborn as sns"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Why Text Can't Be Input Directly\n",
    "example_text = [\"serendipity\", \"cascade\", \"ephemeral\", \"zenith\", \"labyrinth\"]\n",
    "\n",
    "# Try to perform mathematical operations directly on text\n",
    "try:\n",
    "    result = example_text[0] + example_text[1]\n",
    "    print(f\"Adding words: {example_text[0]} + {example_text[1]} = {result}\")\n",
    "except TypeError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    \n",
    "print(\"\"\"\n",
    "Why this doesn't work? \n",
    "\n",
    "1. Text is categorical, not numerical\n",
    "2. No mathematical structure\n",
    "3. Models require inputs for computation\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Show what happens when trying to find similarity\n",
    "print(\"\\nTrying to measure word similarity:\")\n",
    "print(f\"Words: '{example_text[0]}' and '{example_text[1]}'\")\n",
    "print(\"Direct comparison only shows if they're exactly equal:\")\n",
    "print(f\"Are they equal? {example_text[0] == example_text[1]}\")\n",
    "print(\"But we can't measure HOW similar they are!\")\n",
    "\n",
    "# Demonstrate array operations failing\n",
    "print(\"\\nTrying to use in numpy array operations:\")\n",
    "try:\n",
    "    word_array = np.array(example_text)\n",
    "    result = word_array * 2\n",
    "    print(result)\n",
    "except TypeError as e:\n",
    "    print(f\"Error: {e}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 2: Naive Approach - Word to Number Mapping\n",
    "word_to_num = {\n",
    "    \"hello\": 1,\n",
    "    \"hi\": 2,\n",
    "    \"hey\": 3, \n",
    "    \"howdy\": 4,\n",
    "    \"greetings\": 5,\n",
    "    \"welcome\": 6,\n",
    "    \"tree\": 7,\n",
    "    \"flower\": 8,\n",
    "    \"mountain\": 9,\n",
    "    \"river\": 10,\n",
    "    \"ocean\": 11,\n",
    "    \"forest\": 12\n",
    "}\n",
    "\n",
    "def is_greeting(word_num):\n",
    "    return word_num in [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "def is_nature(word_num):\n",
    "    return word_num in [7, 8, 9, 10, 11, 12]\n",
    "\n",
    "def simple_classifier(word_num):\n",
    "    # Simulate a simple model that classifies words\n",
    "    return \"greeting\" if is_greeting(word_num) else \"nature\" if is_nature(word_num) else \"unknown\"\n",
    "\n",
    "# Test the classifier\n",
    "test_words = [\"hello\", \"howdy\", \"tree\", \"ocean\"]\n",
    "print(\"Testing our simple number mapping:\")\n",
    "for word in test_words:\n",
    "    num = word_to_num[word]\n",
    "    classification = simple_classifier(num)\n",
    "    print(f\"{word} (number {num}) -> {classification}\")\n",
    "\n",
    "# Try with an unknown word\n",
    "print(\"\"\"\n",
    "Trying with a new word:\n",
    "'Yo!' -> ???  # 13th most common greeting? https://tandem.net/blog/20-greetings-in-english\n",
    "Problem: Our system breaks with unknown words!\n",
    "\"\"\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Cell 4: Introduction to Word Embeddings\n",
    "# Create some simple 2D embeddings for demonstration\n",
    "simple_embeddings = {\n",
    "    \"hello\":     [0.8, 0.9],\n",
    "    \"hi\":        [0.7, 0.8], \n",
    "    \"hey\":       [0.75, 0.85],\n",
    "    \"greetings\": [0.9, 0.7],\n",
    "    \"howdy\":     [0.85, 0.75],\n",
    "    \"world\":     [-0.5, 0.5],\n",
    "    \"earth\":     [-0.6, 0.4],\n",
    "    \"planet\":    [-0.4, 0.6],\n",
    "    \"tree\":      [-0.8, -0.3],\n",
    "    \"forest\":    [-0.9, -0.4], \n",
    "    \"ocean\":     [-0.7, -0.5],\n",
    "    \"yo\":        [0.65, 0.85],\n",
    "}\n",
    "\n",
    "def plot_embeddings(highlight_word=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for word, coords in simple_embeddings.items():\n",
    "        if word == highlight_word:\n",
    "            plt.scatter(coords[0], coords[1], c='red', s=100)\n",
    "            plt.annotate(word, (coords[0], coords[1]), fontsize=12, color='red')\n",
    "        else:\n",
    "            plt.scatter(coords[0], coords[1])\n",
    "            plt.annotate(word, (coords[0], coords[1]), fontsize=10)\n",
    "    \n",
    "    plt.title(\"Simple 2D Word Embeddings\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Interactive widget to highlight words\n",
    "interact(plot_embeddings, \n",
    "        highlight_word=widgets.Dropdown(\n",
    "            options=[''] + list(simple_embeddings.keys()),\n",
    "            description='Highlight:',\n",
    "            value=''\n",
    "        ))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Cell 5: Similarity in Embedding Space\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "def find_similar_words(word, n=3):\n",
    "    if word not in simple_embeddings:\n",
    "        return \"Word not found in vocabulary\"\n",
    "    \n",
    "    similarities = {}\n",
    "    word_embedding = simple_embeddings[word]\n",
    "    \n",
    "    for other_word, other_embedding in simple_embeddings.items():\n",
    "        if other_word != word:\n",
    "            similarity = cosine_similarity(word_embedding, other_embedding)\n",
    "            similarities[other_word] = similarity\n",
    "    \n",
    "    # Sort by similarity\n",
    "    sorted_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_words[:n]\n",
    "\n",
    "# Interactive similarity finder\n",
    "def show_similarities(word):\n",
    "    similar_words = find_similar_words(word)\n",
    "    print(f\"\\nMost similar words to '{word}':\")\n",
    "    for similar_word, similarity in similar_words:\n",
    "        print(f\"{similar_word}: {similarity:.3f}\")\n",
    "    \n",
    "    # Also show the plot with the word highlighted\n",
    "    plot_embeddings(word)\n",
    "\n",
    "interact(show_similarities, \n",
    "        word=widgets.Dropdown(\n",
    "            options=list(simple_embeddings.keys()),\n",
    "            description='Word:',\n",
    "            value='hello'\n",
    "        ))\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "corpus = [\n",
    "    \"dogs and cats are friendly pets\",\n",
    "    \"pets like dogs need daily walks\",\n",
    "    \"cats and kittens love to play\",\n",
    "    \"puppies and dogs enjoy running\",\n",
    "    \"pets bring joy to families\",\n",
    "    \"cats enjoy sleeping in sunshine\", \n",
    "    \"dogs love playing fetch games\",\n",
    "    \"kittens and puppies are cute\",\n",
    "    \"pets need food and water\",\n",
    "    \"animals like cats and dogs make great companions\"\n",
    "]\n",
    "\n",
    "def create_context_matrix(corpus, window_size=2):\n",
    "    # Create vocabulary \n",
    "    words = ' '.join(corpus).split()\n",
    "    vocab = sorted(list(set(words)))\n",
    "    word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "    \n",
    "    # Initialize context matrix\n",
    "    context_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "    \n",
    "    # Count word co-occurrences with position weighting\n",
    "    for sentence in corpus:\n",
    "        words = sentence.split()\n",
    "        for i, word in enumerate(words):\n",
    "            # Look at surrounding context words\n",
    "            start = max(0, i - window_size)\n",
    "            end = min(len(words), i + window_size + 1)\n",
    "            \n",
    "            for j in range(start, end):\n",
    "                if i != j:\n",
    "                    # Weight by distance - closer words matter more\n",
    "                    distance = abs(i - j)\n",
    "                    weight = 1.0 / distance\n",
    "                    context_matrix[word_to_idx[word]][word_to_idx[words[j]]] += weight\n",
    "    \n",
    "    return context_matrix, vocab\n",
    "\n",
    "def get_similar_words(word, context_matrix, vocab, n=5):\n",
    "    if word not in vocab:\n",
    "        return []\n",
    "    \n",
    "    word_idx = vocab.index(word)\n",
    "    word_vector = context_matrix[word_idx]\n",
    "    \n",
    "    # Calculate cosine similarities\n",
    "    similarities = []\n",
    "    for i, other_word in enumerate(vocab):\n",
    "        if i != word_idx:\n",
    "            other_vector = context_matrix[i]\n",
    "            similarity = np.dot(word_vector, other_vector) / (np.linalg.norm(word_vector) * np.linalg.norm(other_vector))\n",
    "            similarities.append((other_word, similarity))\n",
    "    \n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "def plot_word_similarities(word, context_matrix, vocab):\n",
    "    similar_words = get_similar_words(word, context_matrix, vocab)\n",
    "    if not similar_words:\n",
    "        print(f\"Word '{word}' not found in vocabulary\")\n",
    "        return\n",
    "        \n",
    "    words, scores = zip(*similar_words)\n",
    "    \n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.barh(words, scores)\n",
    "    plt.title(f\"Words most semantically similar to '{word}'\")\n",
    "    plt.xlabel(\"Similarity score\")\n",
    "    plt.show()\n",
    "\n",
    "# Create and display context matrix\n",
    "context_matrix, vocab = create_context_matrix(corpus)\n",
    "\n",
    "# Show heatmap of relationships\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(context_matrix, xticklabels=vocab, yticklabels=vocab)\n",
    "plt.title(\"Word Context Relationships\")\n",
    "plt.show()\n",
    "\n",
    "# Interactive widget to explore semantic similarities\n",
    "interact(\n",
    "    lambda word: plot_word_similarities(word, context_matrix, vocab),\n",
    "    word=widgets.Dropdown(\n",
    "        options=vocab,\n",
    "        description='Word:',\n",
    "        value='dogs'\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\"\"\n",
    "Key Concepts:\n",
    "1. Words are represented by their contexts (surrounding words)\n",
    "2. Similar words appear in similar contexts\n",
    "3. Position and distance between words matters\n",
    "4. This creates meaningful semantic relationships\n",
    "5. Modern embeddings use similar principles but with neural networks\n",
    "\"\"\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Initialize the embedding model\n",
    "\n",
    "def load_model():\n",
    "    return SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to get embeddings and reduce dimensionality\n",
    "\n",
    "def get_3d_embeddings(texts, model):\n",
    "    embeddings = model.encode(texts)\n",
    "    pca = PCA(n_components=3)\n",
    "    embeddings_3d = pca.fit_transform(embeddings)\n",
    "    return embeddings_3d\n",
    "\n",
    "# Function to find similar texts\n",
    "\n",
    "def find_similar_texts(search_text, texts, model, n=5):\n",
    "    all_texts = [search_text] + texts\n",
    "    all_embeddings = model.encode(all_texts)\n",
    "\n",
    "    search_embedding = all_embeddings[0]\n",
    "    text_embeddings = all_embeddings[1:]\n",
    "\n",
    "    similarities = []\n",
    "    for i, embedding in enumerate(text_embeddings):\n",
    "        similarity = np.dot(search_embedding, embedding) / (\n",
    "            np.linalg.norm(search_embedding) * np.linalg.norm(embedding)\n",
    "        )\n",
    "        similarities.append((texts[i], similarity))\n",
    "\n",
    "    return sorted(similarities, key=lambda x: x[1], reverse=True)[:n]\n",
    "\n",
    "# Initial example texts\n",
    "default_texts = [\n",
    "    \"hello\",\n",
    "    \"hi\",\n",
    "    \"hey\",\n",
    "    \"greetings\",\n",
    "    \"world\",\n",
    "    \"earth\",\n",
    "    \"planet\",\n",
    "    \"tree\",\n",
    "    \"forest\",\n",
    "    \"ocean\",\n",
    "    \"yo\",\n",
    "]\n",
    "\n",
    "# Load model\n",
    "embedding_model = load_model()\n",
    "\n",
    "# Create widgets\n",
    "embedding_text_input = widgets.Text(description='Add text:', placeholder='Enter text here')\n",
    "embedding_search_input = widgets.Text(description='Search:', placeholder='Search similar texts')\n",
    "embedding_add_button = widgets.Button(description='Add')\n",
    "embedding_search_button = widgets.Button(description='Search')\n",
    "\n",
    "# Dedicated output areas for this cell so later cells do not interfere\n",
    "embedding_plot_output = widgets.Output()\n",
    "embedding_search_output = widgets.Output()\n",
    "embedding_debug_output = widgets.Output()\n",
    "\n",
    "# Current texts list\n",
    "embedding_texts = default_texts.copy()\n",
    "\n",
    "\n",
    "def log_embedding_message(message):\n",
    "    with embedding_debug_output:\n",
    "        clear_output(wait=True)\n",
    "        print(message)\n",
    "\n",
    "\n",
    "def update_embedding_plot(status_message=None):\n",
    "    with embedding_plot_output:\n",
    "        clear_output(wait=True)\n",
    "        embeddings_3d = get_3d_embeddings(embedding_texts, embedding_model)\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                'text': embedding_texts,\n",
    "                'x': embeddings_3d[:, 0],\n",
    "                'y': embeddings_3d[:, 1],\n",
    "                'z': embeddings_3d[:, 2],\n",
    "            }\n",
    "        )\n",
    "        fig = px.scatter_3d(\n",
    "            df,\n",
    "            x='x',\n",
    "            y='y',\n",
    "            z='z',\n",
    "            text='text',\n",
    "            title='3D Text Embeddings',\n",
    "        )\n",
    "        fig.update_traces(marker=dict(size=8), textposition='top center')\n",
    "        display(fig)\n",
    "        if status_message:\n",
    "            print(status_message)\n",
    "    log_embedding_message(f\"Rendered plot with {len(embedding_texts)} points.\")\n",
    "\n",
    "\n",
    "def on_embedding_add_clicked(_):\n",
    "    new_text = embedding_text_input.value.strip()\n",
    "    if not new_text:\n",
    "        log_embedding_message('Add ignored: input was empty after stripping.')\n",
    "        return\n",
    "    if new_text in embedding_texts:\n",
    "        update_embedding_plot(\"Sentence already plotted.\")\n",
    "    else:\n",
    "        embedding_texts.append(new_text)\n",
    "        update_embedding_plot(f\"Added: {new_text}\")\n",
    "    embedding_text_input.value = ''\n",
    "    with embedding_search_output:\n",
    "        clear_output()\n",
    "\n",
    "\n",
    "def on_embedding_search_clicked(_):\n",
    "    with embedding_search_output:\n",
    "        clear_output()\n",
    "        search_text = embedding_search_input.value.strip()\n",
    "        if not search_text:\n",
    "            log_embedding_message('Search ignored: empty input.')\n",
    "            return\n",
    "        similar = find_similar_texts(search_text, embedding_texts, embedding_model)\n",
    "        print(f\"\\nMost similar texts to '{search_text}':\")\n",
    "        for text, similarity in similar:\n",
    "            print(f\"{text}: {similarity:.3f}\")\n",
    "    log_embedding_message(f\"Ran similarity search for '{search_text}'.\")\n",
    "\n",
    "\n",
    "# Connect button clicks to handlers\n",
    "embedding_add_button.on_click(on_embedding_add_clicked)\n",
    "embedding_search_button.on_click(on_embedding_search_clicked)\n",
    "\n",
    "embedding_controls = widgets.HBox([embedding_text_input, embedding_add_button])\n",
    "embedding_search_controls = widgets.HBox([embedding_search_input, embedding_search_button])\n",
    "\n",
    "embedding_ui = widgets.VBox([\n",
    "    embedding_controls,\n",
    "    embedding_search_controls,\n",
    "    embedding_plot_output,\n",
    "    embedding_search_output,\n",
    "    embedding_debug_output,\n",
    "])\n",
    "\n",
    "display(embedding_ui)\n",
    "\n",
    "update_embedding_plot('Initial projection ready. Add more text or search for similar sentences.')\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Example corpus with diverse examples\n",
    "corpus = [\n",
    "    \"The cat and dog played together in the yard\",\n",
    "    \"The dog and cat were playing in the garden\",  # Similar meaning but different words\n",
    "    \"Machine learning algorithms process data efficiently\",\n",
    "    \"Natural language processing helps computers understand text\",\n",
    "    \"Artificial intelligence is transforming technology\",\n",
    "    \"Deep learning models require large datasets\",\n",
    "    \"The quick brown fox jumps over the lazy dog\",  # Common test sentence\n",
    "    \"AI and ML are revolutionizing industries\",  # Abbreviations vs full words\n",
    "    \"Artificial intelligence and machine learning transform businesses\",  # Same meaning as above\n",
    "    \"The feline and canine were engaging in recreational activities\",  # Formal version of first sentence\n",
    "    \"Data science uses statistical methods\",\n",
    "    \"Statistics and mathematics are used in data analysis\"  # Similar topic, different words\n",
    "]\n",
    "\n",
    "bert_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "\n",
    "def compare_embeddings(corpus):\n",
    "    # TF-IDF Embeddings with smoothing to prevent divide by zero\n",
    "    tfidf = TfidfVectorizer(smooth_idf=True)\n",
    "    tfidf_embeddings = tfidf.fit_transform(corpus).toarray()\n",
    "\n",
    "    bert_embeddings = bert_model.encode(corpus)\n",
    "\n",
    "    # Calculate similarity matrices with error handling\n",
    "    def cosine_similarity_matrix(embeddings):\n",
    "        epsilon = 1e-8\n",
    "        norm = np.linalg.norm(embeddings, axis=1)\n",
    "        norm = np.maximum(norm, epsilon)\n",
    "        normalized = embeddings / norm[:, np.newaxis]\n",
    "        return normalized @ normalized.T\n",
    "\n",
    "    tfidf_sim = cosine_similarity_matrix(tfidf_embeddings)\n",
    "    bert_sim = cosine_similarity_matrix(bert_embeddings)\n",
    "\n",
    "    # Visualize similarities\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    labels = [text[:10] + '...' for text in corpus]\n",
    "\n",
    "    sns.heatmap(\n",
    "        tfidf_sim,\n",
    "        ax=ax1,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='YlOrRd',\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "    )\n",
    "    ax1.set_title('TF-IDF Similarities\\n(Based on word frequency)')\n",
    "\n",
    "    sns.heatmap(\n",
    "        bert_sim,\n",
    "        ax=ax2,\n",
    "        annot=True,\n",
    "        fmt='.2f',\n",
    "        cmap='YlOrRd',\n",
    "        xticklabels=labels,\n",
    "        yticklabels=labels,\n",
    "    )\n",
    "    ax2.set_title('BERT Similarities\\n(Based on semantic meaning)')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "    # Print key differences analysis\n",
    "    print(\"\\nKey Differences Analysis:\")\n",
    "    print(\"\\n1. TF-IDF vs BERT comparison:\")\n",
    "    for i in range(len(corpus)):\n",
    "        for j in range(i + 1, len(corpus)):\n",
    "            diff = abs(bert_sim[i, j] - tfidf_sim[i, j])\n",
    "            if diff > 0.3:\n",
    "                print(f\"\\nDocuments with different similarity scores:\")\n",
    "                print(f\"Doc {i + 1}: {corpus[i]}\")\n",
    "                print(f\"Doc {j + 1}: {corpus[j]}\")\n",
    "                print(f\"TF-IDF similarity: {tfidf_sim[i, j]:.3f}\")\n",
    "                print(f\"BERT similarity: {bert_sim[i, j]:.3f}\")\n",
    "\n",
    "    return tfidf_embeddings, bert_embeddings\n",
    "\n",
    "\n",
    "comparison_notes = \"\"\"Key Differences between TF-IDF and BERT:\n",
    "1. TF-IDF captures word frequency patterns\n",
    "2. BERT captures semantic relationships\n",
    "3. BERT understands word context and order\n",
    "4. TF-IDF is simpler but misses semantic nuances\n",
    "\n",
    "Example: 'AI' and 'artificial intelligence' would be:\n",
    "- Different in TF-IDF (different words)\n",
    "- Similar in BERT (same meaning)\n",
    "\n",
    "Key Observations:\n",
    "\n",
    "1. TF-IDF Embeddings:\n",
    "   - Based on word frequency statistics\n",
    "   - Ignores word order and context\n",
    "   - Treats similar words as different ('AI' vs 'artificial intelligence')\n",
    "   - Fast and memory-efficient\n",
    "   - Good for keyword extraction and basic document similarity\n",
    "\n",
    "2. BERT Embeddings:\n",
    "   - Captures contextual meaning\n",
    "   - Understands semantic relationships\n",
    "   - Recognizes similar concepts even with different words\n",
    "   - More computationally intensive\n",
    "   - Better for understanding language nuances\n",
    "\n",
    "3. When to Use Each:\n",
    "   - TF-IDF: Simple document classification, keyword extraction, search\n",
    "   - BERT: Complex language understanding, semantic similarity, when context matters\n",
    "\"\"\"\n",
    "\n",
    "plot_output = widgets.Output()\n",
    "text_output = widgets.Output()\n",
    "\n",
    "\n",
    "def render_dashboard(status_message=None):\n",
    "    with plot_output:\n",
    "        plot_output.clear_output(wait=True)\n",
    "        compare_embeddings(corpus)\n",
    "    with text_output:\n",
    "        text_output.clear_output(wait=True)\n",
    "        if status_message:\n",
    "            print(status_message)\n",
    "            print()\n",
    "        print(comparison_notes)\n",
    "\n",
    "\n",
    "def update_corpus(text_widget, _):\n",
    "    new_sentence = text_widget.value.strip()\n",
    "    if not new_sentence:\n",
    "        return\n",
    "    if new_sentence in corpus:\n",
    "        status = \"Sentence already in the comparison set.\"\n",
    "    else:\n",
    "        corpus.append(new_sentence)\n",
    "        status = f\"Added: {new_sentence}\"\n",
    "    text_widget.value = ''\n",
    "    render_dashboard(status)\n",
    "\n",
    "\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Enter a new sentence to compare...',\n",
    "    description='Add Text:',\n",
    "    style={'description_width': 'initial'},\n",
    ")\n",
    "\n",
    "add_button = widgets.Button(description='Add', button_style='success')\n",
    "add_button.on_click(lambda b: update_corpus(text_input, b))\n",
    "\n",
    "render_dashboard('Initial comparison rendered. Add more text to update the plots.')\n",
    "\n",
    "control_row = widgets.HBox([text_input, add_button])\n",
    "ui = widgets.VBox([control_row, plot_output, text_output])\n",
    "\n",
    "display(ui)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
